% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mains_function.R
\name{rr.robust}
\alias{rr.robust}
\title{Proximal Gradient Solver for Robust Low-Rank Regression}
\usage{
rr.robust(
  X,
  Y,
  tau = 1,
  lambda = 1,
  penalty = c("SCAD", "MCP", "nuclear"),
  scad_a = 3.7,
  mcp_gamma = 3,
  alpha_in = NULL,
  max_iter = 2000L,
  tol = 1e-07,
  verbose = TRUE,
  svd_rank = NULL,
  warm_start = NULL
)
}
\arguments{
\item{X}{Numeric matrix of predictors of dimension \eqn{n \times p}.}

\item{Y}{Numeric matrix of responses of dimension \eqn{n \times q}.}

\item{tau}{Positive Huber threshold parameter.}

\item{lambda}{Nonnegative regularization parameter.}

\item{penalty}{Character string specifying the spectral penalty.
One of \code{"SCAD"}, \code{"MCP"}, or \code{"nuclear"}.}

\item{scad_a}{SCAD shape parameter (ignored unless \code{penalty = "SCAD"}).}

\item{mcp_gamma}{MCP shape parameter (ignored unless \code{penalty = "MCP"}).}

\item{alpha_in}{Optional step size for the proximal-gradient algorithm.
If \code{NULL}, the step size is chosen internally.}

\item{max_iter}{Maximum number of proximal-gradient iterations.}

\item{tol}{Convergence tolerance for relative change in the objective.}

\item{verbose}{Logical; if \code{TRUE}, progress information is printed.}

\item{svd_rank}{Optional integer specifying a truncated SVD rank
for computational efficiency. If \code{NULL}, a full SVD is used.}

\item{warm_start}{Optional numeric matrix providing an initial value
for \eqn{B}. Must have dimension \eqn{p \times q}.}
}
\value{
A list with components:
\describe{
\item{B}{Estimated coefficient matrix of dimension \eqn{p \times q}.}
\item{objective}{Final value of the objective function.}
\item{iter}{Number of iterations performed.}
\item{converged}{Logical indicating whether convergence was achieved.}
}
}
\description{
Solves a multivariate linear regression problem with Huber loss
and a spectral penalty using a proximal-gradient algorithm.
}
\details{
The optimization problem is
\deqn{
\min_B \; \frac{1}{n} \sum_{i=1}^n \ell_\tau(Y_i - X_i B)
+ \lambda \sum_j r(\sigma_j(B)),
}
where \eqn{\ell_\tau} is the Huber loss and \eqn{r} is a
nonconvex (SCAD, MCP) or convex (nuclear) penalty applied
to the singular values of \eqn{B}.

This function is a thin R wrapper around a compiled C++ routine
accessed via \code{.Call}. All heavy numerical computation is
performed in C++ for efficiency.

Missing values in \code{Y} are allowed and are handled internally.
}
\examples{
\dontrun{
## Simulated robust reduced rank regression with three penalties
set.seed(123)

n <- 200
p <- 12
q <- 7
r_true <- 2

## True low-rank coefficient matrix
U <- matrix(rnorm(p * r_true), p, r_true)
V <- matrix(rnorm(q * r_true), q, r_true)
Btrue <- U \%*\% t(V)

## Design matrix
X <- matrix(rnorm(n * p), n, p)

## Heavy-tailed noise
Y <- X \%*\% Btrue + 1.5 * rt(n * q, df = 3)

## Introduce missing values in Y
missing_rate <- 0.1
idx_na <- sample(seq_len(n * q), floor(missing_rate * n * q))
Y[idx_na] <- NA

## SCAD penalty
fit_scad <- rr.robust( lambda = 0.1 ,
  X = X,   Y = Y,
  penalty = "SCAD",   verbose = FALSE
)

## MCP penalty
fit_mcp <- rr.robust( lambda = 0.1 ,
  X = X,   Y = Y,
  penalty = "MCP",   verbose = FALSE
)

## Nuclear norm penalty
fit_nuclear <- rr.robust( lambda = 0.1 ,
  X = X,  Y = Y,
  penalty = "nuclear",  verbose = FALSE
)

## Frobenius norm estimation errors
c(
  SCAD    = sum((Btrue - fit_scad$B )^2),
  MCP     = sum((Btrue - fit_mcp$B )^2),
  Nuclear = sum((Btrue - fit_nuclear$B )^2)
)
}

}
\seealso{
\code{\link{cv.rr.robust}}
}
