% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mains_function.R
\name{cv.rr.robust}
\alias{cv.rr.robust}
\title{Cross-Validated Robust Reduced Rank Regression}
\usage{
cv.rr.robust(
  X,
  Y,
  penalty = c("SCAD", "MCP", "nuclear"),
  lambda_grid = NULL,
  nfolds = 5,
  seed = 1,
  tau_grid = NULL,
  scad_a = 3.7,
  mcp_gamma = 3,
  alpha_in = NULL,
  max_iter = 2000,
  tol = 1e-05,
  svd_rank = NULL,
  verbose = TRUE
)
}
\arguments{
\item{X}{Numeric design matrix of dimension \eqn{n \times p}, where
rows correspond to observations and columns to predictors.}

\item{Y}{Numeric response matrix of dimension \eqn{n \times q}.
Missing values (\code{NA}) are allowed and are handled internally.}

\item{penalty}{Character string specifying the spectral penalty to use.
Must be one of \code{"SCAD"}, \code{"MCP"}, or \code{"nuclear"}.}

\item{lambda_grid}{Optional numeric vector of candidate regularization
parameters \eqn{\lambda}. If \code{NULL}, a logarithmically spaced
grid is constructed automatically based on the singular values of
\eqn{X^\top Y}.}

\item{nfolds}{Integer giving the number of folds for cross-validation.}

\item{seed}{Integer seed used to generate the cross-validation folds.}

\item{tau_grid}{Optional numeric vector of Huber loss threshold
parameters. If \code{NULL}, a default grid is used.}

\item{scad_a}{Shape parameter for the SCAD penalty. Ignored unless
\code{penalty = "SCAD"}.}

\item{mcp_gamma}{Shape parameter for the MCP penalty. Ignored unless
\code{penalty = "MCP"}.}

\item{alpha_in}{Optional step-size parameter for the proximal-gradient
algorithm. If \code{NULL}, the step size is selected internally by
the solver.}

\item{max_iter}{Maximum number of proximal-gradient iterations used
for each model fit.}

\item{tol}{Convergence tolerance for the proximal-gradient algorithm.}

\item{svd_rank}{Optional integer specifying a truncated SVD rank to
accelerate computation. If \code{NULL}, a full SVD is used.}

\item{verbose}{Logical; if \code{TRUE}, progress information is printed
during cross-validation.}
}
\value{
A list with the following components:
\describe{
\item{best_fit}{The fitted model object returned by
\code{rr.robust} using the optimal tuning
parameters.}
\item{B_best}{Estimated coefficient matrix corresponding to the
optimal model.}
\item{tau_grid}{Vector of Huber loss parameters considered.}
\item{lambda_grid}{Vector of regularization parameters considered.}
\item{cv_loss}{Matrix of cross-validated losses, with rows
corresponding to values in \code{tau_grid} and columns to values
in \code{lambda_grid}.}
\item{best_tau}{Selected value of the Huber loss parameter.}
\item{best_lambda}{Selected value of the regularization parameter.}
\item{best_tau_idx}{Index of \code{best_tau} in \code{tau_grid}.}
\item{best_lambda_idx}{Index of \code{best_lambda} in
\code{lambda_grid}.}
}
}
\description{
Performs K-fold cross-validation for robust reduced rank regression
using a Huber loss and a spectral penalty on the coefficient matrix.
The tuning parameters are selected by minimizing the cross-validated
prediction error.
}
\details{
The underlying estimator solves a multivariate linear regression
problem with coefficient matrix \eqn{B} via a proximal-gradient
algorithm, where robustness is induced through the Huber loss and
low-rank structure is encouraged via a spectral penalty applied to
the singular values of \eqn{B}.

For each combination of \eqn{\tau} in \code{tau_grid} and \eqn{\lambda}
in \code{lambda_grid}, the data are split into \code{nfolds} folds.
The model is trained on \eqn{nfolds - 1} folds and evaluated on the
held-out fold using mean squared prediction error. The reported
cross-validation loss is the average error across folds.

The optimal tuning parameters are selected as the pair
\eqn{(\tau, \lambda)} minimizing the cross-validated loss. A final
model is then refit on the full data using these parameters.
}
\examples{
\dontrun{
## Simulated robust reduced rank regression with three penalties
set.seed(123)

n <- 200
p <- 12
q <- 7
r_true <- 2

## True low-rank coefficient matrix
U <- matrix(rnorm(p * r_true), p, r_true)
V <- matrix(rnorm(q * r_true), q, r_true)
Btrue <- U \%*\% t(V)

## Design matrix
X <- matrix(rnorm(n * p), n, p)

## Heavy-tailed noise
Y <- X \%*\% Btrue + 1.5 * rt(n * q, df = 3)

## Introduce missing values in Y
missing_rate <- 0.2
idx_na <- sample(seq_len(n * q), floor(missing_rate * n * q))
Y[idx_na] <- NA

## SCAD penalty
fit_scad <- cv.rr.robust(
  X = X,   Y = Y,
  penalty = "SCAD",   verbose = FALSE
)

## MCP penalty
fit_mcp <- cv.rr.robust(
  X = X,   Y = Y,
  penalty = "MCP",   verbose = FALSE
)

## Nuclear norm penalty
fit_nuclear <- cv.rr.robust(
  X = X,  Y = Y,
  penalty = "nuclear",  verbose = FALSE
)

## Frobenius norm estimation errors
c(
  SCAD    = sum((Btrue - fit_scad$B_best)^2),
  MCP     = sum((Btrue - fit_mcp$B_best)^2),
  Nuclear = sum((Btrue - fit_nuclear$B_best)^2)
)
}
}
\references{
Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. \emph{Journal of the American
Statistical Association}.

Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax
concave penalty. \emph{Annals of Statistics}.
}
\seealso{
\code{\link{rr.robust}}
}
